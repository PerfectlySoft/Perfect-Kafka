//
//  PerfectKafka.swift
//  Perfect-Kafka
//
//  Created by Rockford Wei on 2017-02-28.
//  Copyright Â© 2017 PerfectlySoft. All rights reserved.
//
//===----------------------------------------------------------------------===//
//
// This source file is part of the Perfect.org open source project
//
// Copyright (c) 2017 - 2018 PerfectlySoft Inc. and the Perfect project authors
// Licensed under Apache License v2.0
//
// See http://perfect.org/licensing.html for license information
//
//===----------------------------------------------------------------------===//
//
#if os(Linux)
    import SwiftGlibc
    import LinuxBridge
    
var errno: Int32 {
    return linux_errno()
    }//end var
#else
    import Darwin
#endif


import ckafka

/// Base Class of Kafka Client. Don't use this class directly! Use Producer or Consumer instead.
public class Kafka {
    
    /// size required for C api string conversion
    public static let szString = 1024
    
    /// Kafka 0.8 only support two types of clients
    public enum `Type` { case PRODUCER, CONSUMER }
    
    /// only effective when initial failure with specific error information
    public enum Failure: Error { case INIT(String) }
    
    /// inner handle for C api
    internal var _handle: OpaquePointer
    
    /// inner record for global config
    internal var _config: Config
    
    /// Instance hash table for management of better and safer C pointer applications and callbacks
    public static var instances:[OpaquePointer: Kafka] = [:]
    
    /// Function type of Error Callback, with the only parameter of error message
    public typealias ErrorCallback = (String) -> Void
    
    /// Error Callback Function. Users should customize this function for error control
    public var OnError:ErrorCallback = { _ in }
    
    
    /// Kafka Errors, directly copy from librdkafka 0.10
    public enum Exception: Int32, Error {
        /* Internal errors to rdkafka: */
        ///  Begin internal error codes
        case _BEGIN = -200
        ///  Received message is incorrect
        case _BAD_MSG = -199
        ///  Bad/unknown compression
        case _BAD_COMPRESSION = -198
        ///  Broker is going away
        case _DESTROY = -197
        ///  Generic failure
        case _FAIL = -196
        ///  Broker transport failure
        case _TRANSPORT = -195
        ///  Critical system resource
        case _CRIT_SYS_RESOURCE = -194
        ///  Failed to resolve broker
        case _RESOLVE = -193
        ///  Produced message timed out
        case _MSG_TIMED_OUT = -192
        /** Reached the end of the topic+partition queue on
         * the broker. Not really an error. */
        case _PARTITION_EOF = -191
        ///  Permanent: Partition does not exist in cluster.
        case _UNKNOWN_PARTITION = -190
        ///  File or filesystem error
        case _FS = -189
        ///  Permanent: Topic does not exist in cluster.
        case _UNKNOWN_TOPIC = -188
        ///  All broker connections are down.
        case _ALL_BROKERS_DOWN = -187
        ///  Invalid argument, or invalid configuration
        case _INVALID_ARG = -186
        ///  Operation timed out
        case _TIMED_OUT = -185
        ///  Queue is full
        case _QUEUE_FULL = -184
        ///  ISR count < required.acks
        case _ISR_INSUFF = -183
        ///  Broker node update
        case _NODE_UPDATE = -182
        ///  SSL error
        case _SSL = -181
        ///  Waiting for coordinator to become available.
        case _WAIT_COORD = -180
        ///  Unknown client group
        case _UNKNOWN_GROUP = -179
        ///  Operation in progress
        case _IN_PROGRESS = -178
        ///  Previous operation in progress, wait for it to finish.
        case _PREV_IN_PROGRESS = -177
        ///  This operation would interfere with an existing subscription
        case _EXISTING_SUBSCRIPTION = -176
        ///  Assigned partitions (rebalance_cb)
        case _ASSIGN_PARTITIONS = -175
        ///  Revoked partitions (rebalance_cb)
        case _REVOKE_PARTITIONS = -174
        ///  Conflicting use
        case _CONFLICT = -173
        ///  Wrong state
        case _STATE = -172
        ///  Unknown protocol
        case _UNKNOWN_PROTOCOL = -171
        ///  Not implemented
        case _NOT_IMPLEMENTED = -170
        ///  Authentication failure
        case _AUTHENTICATION = -169
        ///  No stored offset
        case _NO_OFFSET = -168
        ///  Outdated
        case _OUTDATED = -167
        ///  Timed out in queue
        case _TIMED_OUT_QUEUE = -166
        ///  Feature not supported by broker
        case _UNSUPPORTED_FEATURE = -165
        ///  Awaiting cache update
        case _WAIT_CACHE = -164
        
        ///  End internal error codes
        case _END = -100
        
        /* Kafka broker errors: */
        ///  Unknown broker error
        case UNKNOWN = -1
        ///  Success
        case NO_ERROR = 0
        ///  Offset out of range
        case OFFSET_OUT_OF_RANGE = 1
        ///  Invalid message
        case INVALID_MSG = 2
        ///  Unknown topic or partition
        case UNKNOWN_TOPIC_OR_PART = 3
        ///  Invalid message size
        case INVALID_MSG_SIZE = 4
        ///  Leader not available
        case LEADER_NOT_AVAILABLE = 5
        ///  Not leader for partition
        case NOT_LEADER_FOR_PARTITION = 6
        ///  Request timed out
        case REQUEST_TIMED_OUT = 7
        ///  Broker not available
        case BROKER_NOT_AVAILABLE = 8
        ///  Replica not available
        case REPLICA_NOT_AVAILABLE = 9
        ///  Message size too large
        case MSG_SIZE_TOO_LARGE = 10
        ///  StaleControllerEpochCode
        case STALE_CTRL_EPOCH = 11
        ///  Offset metadata string too large
        case OFFSET_METADATA_TOO_LARGE = 12
        ///  Broker disconnected before response received
        case NETWORK_EXCEPTION = 13
        ///  Group coordinator load in progress
        case GROUP_LOAD_IN_PROGRESS = 14
        ///  Group coordinator not available
        case GROUP_COORDINATOR_NOT_AVAILABLE = 15
        ///  Not coordinator for group
        case NOT_COORDINATOR_FOR_GROUP = 16
        ///  Invalid topic
        case TOPIC_EXCEPTION = 17
        ///  Message batch larger than configured server segment size
        case RECORD_LIST_TOO_LARGE = 18
        ///  Not enough in-sync replicas
        case NOT_ENOUGH_REPLICAS = 19
        ///  Message(s) written to insufficient number of in-sync replicas
        case NOT_ENOUGH_REPLICAS_AFTER_APPEND = 20
        ///  Invalid required acks value
        case INVALID_REQUIRED_ACKS = 21
        ///  Specified group generation id is not valid
        case ILLEGAL_GENERATION = 22
        ///  Inconsistent group protocol
        case INCONSISTENT_GROUP_PROTOCOL = 23
        ///  Invalid group.id
        case INVALID_GROUP_ID = 24
        ///  Unknown member
        case UNKNOWN_MEMBER_ID = 25
        ///  Invalid session timeout
        case INVALID_SESSION_TIMEOUT = 26
        ///  Group rebalance in progress
        case REBALANCE_IN_PROGRESS = 27
        ///  Commit offset data size is not valid
        case INVALID_COMMIT_OFFSET_SIZE = 28
        ///  Topic authorization failed
        case TOPIC_AUTHORIZATION_FAILED = 29
        ///  Group authorization failed
        case GROUP_AUTHORIZATION_FAILED = 30
        ///  Cluster authorization failed
        case CLUSTER_AUTHORIZATION_FAILED = 31
        ///  Invalid timestamp
        case INVALID_TIMESTAMP = 32
        ///  Unsupported SASL mechanism
        case UNSUPPORTED_SASL_MECHANISM = 33
        ///  Illegal SASL state
        case ILLEGAL_SASL_STATE = 34
        ///  Unuspported version
        case UNSUPPORTED_VERSION = 35
        ///  Topic already exists
        case TOPIC_ALREADY_EXISTS = 36
        ///  Invalid number of partitions
        case INVALID_PARTITIONS = 37
        ///  Invalid replication factor
        case INVALID_REPLICATION_FACTOR = 38
        ///  Invalid replica assignment
        case INVALID_REPLICA_ASSIGNMENT = 39
        ///  Invalid config
        case INVALID_CONFIG = 40
        ///  Not controller for cluster
        case NOT_CONTROLLER = 41
        ///  Invalid request
        case INVALID_REQUEST = 42
        ///  Message format on broker does not support request
        case UNSUPPORTED_FOR_MESSAGE_FORMAT = 43
        /// All other errors
        case END_ALL = 44
    }//end
    
    /// Topic Configuration
    public class TopicConfig {
        
        /// inner handle for C api
        internal var conf: OpaquePointer
        
        /// constructor of Topic Configuration
        /// - parameters:
        ///   - configuration: TopicConfig? .  nil for new configuration; or duplicate it if not nil
        public init (_ configuration: TopicConfig? = nil) throws {
            
            // check if the original configuration is available
            if let config = configuration {
                
                // if available, duplicate it.
                guard let cnf = rd_kafka_topic_conf_dup(config.conf) else {
                    throw Exception.UNKNOWN
                }//end guard
                
                // set handler
                conf = cnf
            }else {
                
                // if not avaialble, create a new one.
                guard let cnf = rd_kafka_topic_conf_new() else {
                    throw Exception.UNKNOWN
                }//end guard
                
                // set handler
                conf = cnf
            }//end if
        }//end init
        
        deinit {
            // don't destroy it, producer / consumer will release it themselves.
            //rd_kafka_topic_conf_destroy(conf)
        }//end deconstruction
        
        /// list all keys and values in a configuration as a dictionary, read only.
        public var properties: [String: String] {
            get {
                
                // prepare an empty dictionary for the configuration.
                var dic: [String:String] = [:]
                var cnt = 0
                
                // load the configuration into a C pointer array
                guard let array = rd_kafka_topic_conf_dump(conf, &cnt) else {
                    return dic
                }//end guard
                
                // return if empty
                if cnt < 1 {
                    return dic
                }//end if
                
                // key and values are pair in the same C pointer array
                for i in 0 ... cnt / 2 {
                    guard let k = array.advanced(by: i * 2).pointee,
                        let v = array.advanced(by: i * 2 + 1).pointee
                        else {
                            break
                    }//end guard
                    
                    // save the key & value to the new dictionary
                    let key = String(cString: k)
                    let value = String(cString: v)
                    dic[key] = value
                }//next
                
                // release resources.
                rd_kafka_conf_dump_free(array, cnt)
                
                // return the new created dictionary
                return dic
            }//end get
        }//end properties
        
        /// get a variable value from the current configuration
        /// - parameters:
        ///   - variable: String for a valid variable name
        /// - returns:
        ///   value as String.
        /// - throws:
        ///   Exception
        public func `get` (_ variable: String) throws -> String {
            guard let value = properties[variable] else { throw Exception.UNKNOWN }
            return value
        }//end get
        
        /// set a variable value to the current configuration
        /// - parameters:
        ///   - variable: String for a valid variable name
        ///   - value: String value to set of this variable
        /// - throws:
        ///   Exception
        public func `set` (_ variable: String, value: String) throws {
            let reason = rd_kafka_topic_conf_set(conf, variable, value, nil, 0)
            guard reason.rawValue == Exception.NO_ERROR.rawValue else {
                throw Exception(rawValue: reason.rawValue) ?? Exception.UNKNOWN
            }//end guard
        }//end set
    }
    
    /// Global Configuration Class
    public class Config {
        
        /// internal handle for C api
        internal var conf: OpaquePointer
        
        /// constructor of configuration class
        /// - parameters:
        ///   - configuration: Config? nil for creating new instance, otherwise will duplicate
        /// - throws:
        ///   Exception
        public init (_ configuration: Config? = nil) throws {
            
            // check if not nil
            if let config = configuration {
                
                // duplicate the current instance
                guard let cnf = rd_kafka_conf_dup(config.conf) else {
                    throw Exception.UNKNOWN
                }//end guard
                
                // set the handler
                conf = cnf
            }else {
                
                // otherwise create a new instance
                guard let cnf = rd_kafka_conf_new() else {
                    throw Exception.UNKNOWN
                }//end guard
                
                // set the handler
                conf = cnf
            }//end if
        }//end init
        
        /// deconstructor
        deinit {
            // don't destroy it, producer / consumer will release it themselves.
            // rd_kafka_conf_destroy(conf)
        }//end deconstruction
        
        /// list all keys and values in a configuration as a dictionary, read only.
        public var properties: [String: String] {
            get {
                
                // prepare an empty dictionary
                var dic: [String:String] = [:]
                var cnt = 0
                
                // retrieve all variables
                guard let array = rd_kafka_conf_dump(conf, &cnt) else {
                    return dic
                }//end guard
                if cnt < 1 {
                    return dic
                }//end if
                
                // save all keys and values into the dictionary
                for i in 0 ... cnt / 2 {
                    guard let k = array.advanced(by: i * 2).pointee,
                        let v = array.advanced(by: i * 2 + 1).pointee
                        else {
                            break
                    }//end guard
                    let key = String(cString: k)
                    let value = String(cString: v)
                    dic[key] = value
                }//next
                rd_kafka_conf_dump_free(array, cnt)
                return dic
            }//end get
        }//end properties
        
        /// get a variable value from the current configuration
        /// - parameters:
        ///   - variable: String for a valid variable name
        /// - returns:
        ///   value as String.
        /// - throws:
        ///   Exception
        public func `get` (_ variable: String) throws -> String {
            /*
             The following code is reserved for librdkafka 0.9 and above
             until Feb 28, 2017, Ubuntu 16.04 distribution is only 0.8
             watch for updates; if librdkafka-dev was upgraded, then please remove
             the linux conditional code
             */
            #if os(Linux)
                guard let value = properties[variable] else {
                    throw Exception.UNKNOWN
                }//end guard
                return value
            #else
                var size = Kafka.szString
                let value = UnsafeMutablePointer<CChar>.allocate(capacity: size)
                let reason = rd_kafka_conf_get(conf, variable, value, &size)
                guard reason.rawValue == Exception.NO_ERROR.rawValue else {
                    value.deallocate(capacity: Kafka.szString)
                    throw Exception(rawValue: reason.rawValue) ?? Exception.UNKNOWN
                }//end guard
                let valueString = String(cString: value)
                value.deallocate(capacity: Kafka.szString)
                return valueString
            #endif
        }//end get
        
        /// set a variable value to the current configuration
        /// - parameters:
        ///   - variable: String for a valid variable name
        ///   - value: String value to set of this variable
        /// - throws:
        ///   Exception
        public func `set` (_ variable: String, value: String) throws {
            let reason = rd_kafka_conf_set(conf, variable, value, nil, 0)
            guard reason.rawValue == Exception.NO_ERROR.rawValue else {
                throw Exception(rawValue: reason.rawValue) ?? Exception.UNKNOWN
            }//end guard
        }//end set
    }//end Config
    
    /// constructor of Kafka client base class
    /// - parameters:
    ///   - type: Type of Kafka client, must be either producer or consumer
    ///   - config: Config? global configuration, nil for new instance creation (default)
    public init(type: `Type`, config: Config? = nil) throws {
        
        // determine the client type
        let kType = type == .PRODUCER ? RD_KAFKA_PRODUCER : RD_KAFKA_CONSUMER
        
        // check if need to create new configuration by default
        if config == nil {
            _config = try Config()
        }else {
            _config = config!
        }//end if
        
        // create new client instance
        let errstr = UnsafeMutablePointer<CChar>.allocate(capacity: Kafka.szString)
        guard let h = rd_kafka_new(kType, _config.conf, errstr, Kafka.szString) else {
            let e = String(cString: errstr)
            errstr.deallocate(capacity: Kafka.szString)
            throw Failure.INIT(e)
        }//end guard
        errstr.deallocate(capacity: Kafka.szString)
        _handle = h
        
    }//end init
    
    /// destructor of Kafka base client
    deinit { rd_kafka_destroy(_handle) }
    
    /// name of the client
    public var name: String {
        get {
            guard let n = rd_kafka_name(_handle) else { return "" }
            return String(cString: n)
        }//end get
    }//end name
    
    /// connect to brokers
    /// - parameter:
    ///   - brokers: String, in form of "host:port,...", e.g., "host1:9092,host2:9092,host3:9092"
    /// - returns:
    ///   quantity of brokers that connected.
    public func connect(brokers: String) -> Int {
        return Int(rd_kafka_brokers_add(_handle, brokers))
    }//end add
    
    /// connect to brokers
    /// - parameter:
    ///   - brokers: [String], in form of ["host:port"], e.g., ["host1:9092","host2:9092","host3:9092"]
    /// - returns:
    ///   quantity of brokers that connected.
    public func connect(brokers: [String]) -> Int {
        return connect(brokers: brokers.joined(separator: ","))
    }//end func add
    
    /// connect to brokers
    /// - parameter:
    ///   - brokers: [String: Int], in form of ["host":port], e.g., ["host1":9092,"host2":9092,"host3":9092]
    /// - returns:
    ///   quantity of brokers that connected.
    public func connect(brokers: [String: Int]) -> Int {
        return connect(brokers: brokers.reduce("") { $0.isEmpty ? "\($1.key):\($1.value)" : "\($0),\($1.key):\($1.value)" })
    }//end func add
    
    /// Broker structure, a part of MetaData
    public struct Broker {
        /// Broker Id
        public var id = 0
        /// Host name of the broker
        public var host = ""
        /// Host port that listens
        public var port = 0
    }//end Broker
    
    /// Partition structure, a part of MetaData
    public struct Partition {
        /// Partition Id
        public var id = 0
        /// Partition error reported by broker
        public var err = Exception.NO_ERROR
        /// Leader broker
        public var leader = 0
        /// Replica brokers
        public var replicas = [Int]()
        /// In-Sync-Replica brokers
        public var isrs = [Int]()
    }//end Partition
    
    /// Topic structure, a part of MetaData
    public struct Topic {
        /// Topic name
        public var name = ""
        /// Topic error reported by broker
        public var err = Exception.NO_ERROR
        /// Partitions of this topic
        public var partitions = [Partition]()
    }//end Topic
    
    /// Meta data for broker report
    public struct MetaData {
        /// Brokers that connected
        public var brokers = [Broker]()
        /// Available topics
        public var topics = [Topic]()
        /// Broker originating this metadata
        public var origBrokerId = 0
        /// Name of originating broker
        public var origBrokerName = ""
    }//end MetaData
    
    /// internal function for getting meta data from a broker
    /// - parameters:
    ///   - topicHandle: OpaquePointer?, nil for all topics, otherwise for specific topic.
    ///   - timeout: UInt, timeout for retrieving information, in milli seconds. Default is 1 second.
    /// - returns
    ///   MetaData
    /// - throws
    ///   Exception
    @discardableResult
    internal func getBrokerInfo(topicHandle: OpaquePointer?, timeout: UInt = 1000) throws -> MetaData {
        
        // prepare an empty meta data container
        var m = MetaData()
        
        // prepare the pointer to hold the data returned
        let ppMeta = UnsafeMutablePointer<UnsafePointer<rd_kafka_metadata>?>.allocate(capacity: 1)
        
        // C api return value
        var reason: rd_kafka_resp_err_t
        
        // if handle is valid, then try to gather information only for this specific handle
        if let handle = topicHandle {
            
            // get meta data by assigning the specific topic handle
            reason = rd_kafka_metadata(_handle, 0, handle, ppMeta, Int32(timeout))
        } else {
            
            // otherwise should collect all possible topics from the same broker
            reason = rd_kafka_metadata(_handle, 1, nil, ppMeta, Int32(timeout))
        }//end if
        
        // check return value for errors
        guard reason.rawValue == Exception.NO_ERROR.rawValue else {
            ppMeta.deallocate(capacity: 1)
            throw Exception(rawValue: reason.rawValue) ?? Exception.UNKNOWN
        }//end guard
        
        // get the newly allocated meta data pointer
        guard let pMeta = ppMeta.pointee else {
            throw Exception.UNKNOWN
        }//end guard
        
        // get the meta data structure
        let d = pMeta.pointee
        
        // translate C structure to Swift structure
        
        // set originated broker id
        m.origBrokerId = Int(d.orig_broker_id)
        
        // set originated broker name
        m.origBrokerName = String(cString: d.orig_broker_name)
        
        // save broker info into an array
        for i in 0 ... d.broker_cnt - 1 {
            
            // jump to the right pointer
            let b = d.brokers.advanced(by: Int(i)).pointee
            let broker = Broker(id: Int(b.id), host: String(cString:b.host), port: Int(b.port))
            m.brokers.append(broker)
        }//next i
        
        // walk through all topics
        for i in 0 ... d.topic_cnt - 1 {
            
            // jump to the right pointer
            let t = d.topics.advanced(by: Int(i)).pointee
            
            // create an empty topic to store data
            var topic = Topic()
            
            // set topic basic info
            topic.name = String(cString: t.topic)
            topic.err = Exception(rawValue: t.err.rawValue) ?? Exception.UNKNOWN
            
            // get all partitions for the specific topic
            for j in 0 ... t.partition_cnt - 1 {
                let p = t.partitions.advanced(by: Int(j)).pointee
                
                // create an empty partition to store data
                var part = Partition()
                
                // get partition id
                part.id = Int(p.id)
                
                // get partition errors
                part.err = Exception(rawValue: p.err.rawValue) ?? Exception.UNKNOWN
                
                // get partition leader
                part.leader = Int(p.leader)
                
                // get partition replicas
                for k in 0 ... p.replica_cnt - 1 {
                    let replica = p.replicas.advanced(by: Int(k)).pointee
                    part.replicas.append(Int(replica))
                }//next k
                
                // get ISR brokers in this partition
                for k in 0 ... p.isr_cnt - 1 {
                    let isr = p.isrs.advanced(by: Int(k)).pointee
                    part.isrs.append(Int(isr))
                }//next i
                
                // save partition info into topic data structure
                topic.partitions.append(part)
            }//next j
            
            // add the newly allocated topic to meta data return struture
            m.topics.append(topic)
        }//next i
        
        // release intermediate resources
        rd_kafka_metadata_destroy(pMeta)
        ppMeta.deallocate(capacity: 1)
        
        // return the meta data expected
        return m
    }//end fun
}//end class
